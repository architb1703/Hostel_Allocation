{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single_song.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhfdDg7P0XwVmUVflXr7Op",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architb1703/Hostel_Allocation/blob/master/Single_song.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB0gqMgYk3FB"
      },
      "source": [
        "This file contains the code for experimenting with HITL adaptation on a single songs with only part of the music track annotated by the user. The purpose of this experiment was to check if our approach could be used for real-time audio editting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCxyYJNE6sc-",
        "outputId": "d591142e-7eb4-4298-a642-f796abd1b753"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwG80GRR61FM",
        "outputId": "9ee64d77-4911-4b62-f248-ab9cedd8258f"
      },
      "source": [
        "!pip install stempeg\n",
        "!pip install mir_eval\n",
        "!pip install museval\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import stempeg as st\n",
        "import mir_eval\n",
        "import pickle\n",
        "import museval\n",
        "np.random.seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stempeg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/e5/84adc8506b61ca9f205d9dcc5558b6b5b1fa477c45616f553a0ca1b8020d/stempeg-0.2.3-py3-none-any.whl (963kB)\n",
            "\r\u001b[K     |▍                               | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 19.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 20.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 18.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 11.8MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 11.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 12.2MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 194kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 204kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 215kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 225kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 235kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 245kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 256kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 266kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 276kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 286kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 296kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 307kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 317kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 327kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 337kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 348kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 368kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 378kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 389kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 399kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 409kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 419kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 430kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 440kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 450kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 460kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 471kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 481kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 491kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 501kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 512kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 522kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 532kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 542kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 552kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 563kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 573kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 583kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 593kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 604kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 614kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 624kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 634kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 645kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 655kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 665kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 675kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 686kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 696kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 706kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 716kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 727kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 737kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 747kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 757kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 768kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 778kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 788kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 798kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 808kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 819kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 829kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 839kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 849kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 860kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 870kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 880kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 890kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 901kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 911kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 921kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 931kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 942kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 952kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 962kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 972kB 9.8MB/s \n",
            "\u001b[?25hCollecting ffmpeg-python>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from stempeg) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python>=0.2.0->stempeg) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python, stempeg\n",
            "Successfully installed ffmpeg-python-0.2.0 stempeg-0.2.3\n",
            "Collecting mir_eval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/fe/be4f7a59ed71938e21e89f23afe93eea0d39eb3e77f83754a12028cf1a68/mir_eval-0.6.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.15.0)\n",
            "Building wheels for collected packages: mir-eval\n",
            "  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.6-cp37-none-any.whl size=96515 sha256=5895794c10253a6cb0709d7f31321e6625bb4c70497e39da0435ee633fe41cd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/ce/30/730fa72addf275e49d90683b01b3613048b4be3bf7ff8eb6ec\n",
            "Successfully built mir-eval\n",
            "Installing collected packages: mir-eval\n",
            "Successfully installed mir-eval-0.6\n",
            "Collecting museval\n",
            "  Downloading https://files.pythonhosted.org/packages/37/b2/00c392d76073408fcc9dbd6cafcd6c70d5f289fda5c29ea909c52ef80f80/museval-0.4.0-py2.py3-none-any.whl\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/04/377418ac1e530ce2a196b54c6552c018fdf1fe776718053efb1f216bffcd/simplejson-3.17.2-cp37-cp37m-manylinux2010_x86_64.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from museval) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from museval) (1.4.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from museval) (0.10.3.post1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from museval) (2.6.0)\n",
            "Collecting musdb>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/08/5500771218e9725572e82ac555b49d45336a16611febc9f9ba2395e9e9b4/musdb-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from museval) (1.1.5)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->museval) (1.14.5)\n",
            "Requirement already satisfied: stempeg>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from musdb>=0.4.0->museval) (0.2.3)\n",
            "Collecting pyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from musdb>=0.4.0->museval) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->museval) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->museval) (2.8.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->museval) (2.20)\n",
            "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from stempeg>=0.2.3->musdb>=0.4.0->museval) (0.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml->musdb>=0.4.0->museval) (3.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->museval) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb>=0.4.0->museval) (0.16.0)\n",
            "Installing collected packages: simplejson, pyaml, musdb, museval\n",
            "Successfully installed musdb-0.4.0 museval-0.4.0 pyaml-20.4.0 simplejson-3.17.2\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaDWSL9662F9",
        "outputId": "87602e49-043f-4ce0-f16a-6e857b738c4b"
      },
      "source": [
        "basepath = '/content/drive/MyDrive/UGP/UGP/ugp data/train/'\n",
        "from shutil import copyfile\n",
        "from joblib import Parallel, delayed\n",
        "try:\n",
        "  os.mkdir('./data/')\n",
        "except:\n",
        "  pass\n",
        "files = []\n",
        "for f in os.listdir(basepath):\n",
        "  if(f.split('.')[-1]=='npz'):\n",
        "    files.append(f)\n",
        "Parallel(n_jobs=8)(delayed(lambda x : copyfile(basepath+x, './data/'+x))(x) for x in files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./data/Clara Berry And Wooldog - Air Traffic.npz',\n",
              " './data/Clara Berry And Wooldog - Stella.npz',\n",
              " './data/Clara Berry And Wooldog - Waltz For My Victims.npz',\n",
              " './data/Cnoc An Tursa - Bannockburn.npz',\n",
              " './data/Creepoid - OldTree.npz',\n",
              " './data/Dark Ride - Burning Bridges.npz',\n",
              " './data/Dreamers Of The Ghetto - Heavy Love.npz',\n",
              " './data/Drumtracks - Ghost Bitch.npz',\n",
              " './data/Faces On Film - Waiting For Ga.npz',\n",
              " './data/Fergessen - Back From The Start.npz',\n",
              " './data/Fergessen - Nos Palpitants.npz',\n",
              " './data/Fergessen - The Wind.npz',\n",
              " './data/Flags - 54.npz',\n",
              " './data/Giselle - Moss.npz',\n",
              " './data/Grants - PunchDrunk.npz',\n",
              " './data/Helado Negro - Mitad Del Mundo.npz',\n",
              " './data/Hezekiah Jones - Borrowed Heart.npz',\n",
              " './data/Hollow Ground - Left Blind.npz',\n",
              " './data/Hop Along - Sister Cities.npz',\n",
              " './data/Invisible Familiars - Disturbing Wildlife.npz',\n",
              " './data/James May - All Souls Moon.npz',\n",
              " './data/James May - Dont Let Go.npz',\n",
              " './data/James May - If You Say.npz',\n",
              " './data/James May - On The Line.npz',\n",
              " './data/Jay Menon - Through My Eyes.npz',\n",
              " './data/Johnny Lokke - Promises & Lies.npz',\n",
              " './data/Johnny Lokke - Whisper To A Scream.npz',\n",
              " './data/Jokers, Jacks & Kings - Sea Of Leaves.npz',\n",
              " './data/Leaf - Come Around.npz',\n",
              " './data/Leaf - Summerghost.npz',\n",
              " './data/Leaf - Wicked.npz',\n",
              " './data/Lushlife - Toynbee Suite.npz',\n",
              " './data/Matthew Entwistle - Dont You Ever.npz',\n",
              " './data/Meaxic - Take A Step.npz',\n",
              " './data/Meaxic - You Listen.npz',\n",
              " './data/Music Delta - 80s Rock.npz',\n",
              " './data/Music Delta - Beatles.npz',\n",
              " './data/Music Delta - Britpop.npz',\n",
              " './data/Music Delta - Country1.npz',\n",
              " './data/Music Delta - Country2.npz',\n",
              " './data/Music Delta - Disco.npz',\n",
              " './data/Music Delta - Gospel.npz',\n",
              " './data/Music Delta - Grunge.npz',\n",
              " './data/Music Delta - Hendrix.npz',\n",
              " './data/Music Delta - Punk.npz',\n",
              " './data/Music Delta - Reggae.npz',\n",
              " './data/Music Delta - Rock.npz',\n",
              " './data/Music Delta - Rockabilly.npz',\n",
              " './data/Night Panther - Fire.npz',\n",
              " './data/North To Alaska - All The Same.npz',\n",
              " './data/Patrick Talbot - A Reason To Leave.npz',\n",
              " './data/Patrick Talbot - Set Me Free.npz',\n",
              " \"./data/Phre The Eon - Everybody's Falling Apart.npz\",\n",
              " './data/Port St Willow - Stay Even.npz',\n",
              " './data/Remember December - C U Next Time.npz',\n",
              " './data/Secret Mountains - High Horse.npz',\n",
              " './data/Skelpolu - Human Mistakes.npz',\n",
              " './data/Skelpolu - Together Alone.npz',\n",
              " './data/Snowmine - Curfews.npz',\n",
              " \"./data/Spike Mullings - Mike's Sulking.npz\",\n",
              " './data/St Vitus - Word Gets Around.npz',\n",
              " './data/Steven Clark - Bounty.npz',\n",
              " './data/Strand Of Oaks - Spacestation.npz',\n",
              " './data/Sweet Lights - You Let Me Down.npz',\n",
              " './data/Swinging Steaks - Lost My Way.npz',\n",
              " './data/The Districts - Vermont.npz',\n",
              " './data/The Long Wait - Back Home To Blue.npz',\n",
              " './data/The Scarlet Brand - Les Fleurs Du Mal.npz',\n",
              " \"./data/The Wrong'Uns - Rothko.npz\",\n",
              " './data/Tim Taler - Stalker.npz',\n",
              " './data/Titanium - Haunted Age.npz',\n",
              " './data/Traffic Experiment - Once More (With Feeling).npz',\n",
              " './data/Traffic Experiment - Sirens.npz',\n",
              " './data/Triviul - Angelsaint.npz',\n",
              " './data/Triviul - Dorothy.npz',\n",
              " './data/Voelund - Comfort Lives In Belief.npz',\n",
              " './data/Wall Of Death - Femme.npz',\n",
              " './data/Young Griffo - Blood To Bone.npz',\n",
              " './data/Young Griffo - Facade.npz',\n",
              " './data/Young Griffo - Pennies.npz',\n",
              " './data/Auctioneer - Our Future Faces.npz',\n",
              " './data/A Classic Education - NightOwl.npz',\n",
              " './data/ANiMAL - Rockshow.npz',\n",
              " './data/Actions - South Of The Water.npz',\n",
              " './data/asshat.npz',\n",
              " './data/BigTroubles - Phantom.npz',\n",
              " './data/Chris Durban - Celebrate.npz',\n",
              " './data/Angela Thomas Wade - Milk Cow Blues.npz',\n",
              " './data/Celestial Shore - Die For Us.npz',\n",
              " './data/Actions - One Minute Smile.npz',\n",
              " './data/ANiMAL - Clinic A.npz',\n",
              " './data/Atlantis Bound - It Was My Fault For Waiting.npz',\n",
              " './data/Bill Chudziak - Children Of No-one.npz',\n",
              " './data/AvaLuna - Waterduct.npz',\n",
              " './data/Aimee Norwich - Child.npz',\n",
              " './data/Alexander Ross - Goodbye Bolero.npz',\n",
              " './data/ANiMAL - Easy Tiger.npz',\n",
              " './data/Black Bloc - If You Want Success.npz',\n",
              " './data/Alexander Ross - Velvet Curtain.npz',\n",
              " \"./data/Actions - Devil's Words.npz\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGP9cFAB7AbP",
        "outputId": "eb2db829-5ec8-4207-d489-87f479bc45b6"
      },
      "source": [
        "basepath = '/content/drive/MyDrive/UGP/'\n",
        "from shutil import copyfile\n",
        "from joblib import Parallel, delayed\n",
        "try:\n",
        "  os.mkdir('./data/')\n",
        "except:\n",
        "  pass\n",
        "files = []\n",
        "for f in os.listdir(basepath):\n",
        "  if(f.split('.')[-1]=='npz'):\n",
        "    files.append(f)\n",
        "Parallel(n_jobs=8)(delayed(lambda x : copyfile(basepath+x, './data/'+x))(x) for x in files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./data/hitl0.npz',\n",
              " './data/hitl1.npz',\n",
              " './data/hitl2.npz',\n",
              " './data/hitl3.npz',\n",
              " './data/hitl4.npz',\n",
              " './data/hitl5.npz',\n",
              " './data/hitl6.npz',\n",
              " './data/hitl7.npz',\n",
              " './data/hitl8.npz',\n",
              " './data/hitl9.npz',\n",
              " './data/hitl10.npz',\n",
              " './data/hitl11.npz',\n",
              " './data/hitl12.npz',\n",
              " './data/hitl13.npz',\n",
              " './data/hitl14.npz',\n",
              " './data/hitl15.npz',\n",
              " './data/hitl16.npz',\n",
              " './data/hitl17.npz',\n",
              " './data/hitl18.npz',\n",
              " './data/hitl19.npz',\n",
              " './data/hitl20.npz',\n",
              " './data/hitl21.npz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwh3QQEO66-p",
        "outputId": "9ce937b6-22c7-496f-a136-e22dcd60dade"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "class Source_Pitch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Source_Pitch, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, 16, (5,5), (2,2), (2,2))\n",
        "    self.conv2 = nn.Conv2d(16, 32, (5,5), (2,2), (2,2))\n",
        "    self.conv3 = nn.Conv2d(32, 64, (5,5), (2,2), (2,2))\n",
        "    self.conv4 = nn.Conv2d(64, 128, (5,5), (2,2), (2,2))\n",
        "    self.conv5 = nn.Conv2d(128, 256, (5,5), (2,2), (2,2))\n",
        "    self.conv6 = nn.Conv2d(256, 512, (5,5), (2,2), (2,2))\n",
        "\n",
        "    self.conv7 = nn.Conv2d(512, 256, (5,5), (1,1), (2,2))\n",
        "    self.conv8 = nn.Conv2d(256, 128, (5,5), (1,1), (2,2))\n",
        "    self.conv9 = nn.Conv2d(128, 64, (5,5), (1,1), (2,2))\n",
        "    self.conv10 = nn.Conv2d(64, 32, (5,5), (1,1), (2,2))\n",
        "    self.conv11 = nn.Conv2d(32, 16, (5,5), (1,1), (2,2))\n",
        "    \n",
        "\n",
        "    self.convT1 = nn.ConvTranspose2d(512, 256, (5,5), (2,2), (2,2))\n",
        "    self.convT2 = nn.ConvTranspose2d(256, 128, (5,5), (2,2), (2,2))\n",
        "    self.convT3 = nn.ConvTranspose2d(128, 64, (5,5), (2,2), (2,2))\n",
        "    self.convT4 = nn.ConvTranspose2d(64, 32, (5,5), (2,2), (2,2))\n",
        "    self.convT5 = nn.ConvTranspose2d(32, 16, (5,5), (2,2), (2,2))\n",
        "    self.convT6 = nn.ConvTranspose2d(16, 1, (5,5), (2,2), (2,2))\n",
        "\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.bn0 = nn.BatchNorm2d(1)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.bn4 = nn.BatchNorm2d(128)\n",
        "    self.bn5 = nn.BatchNorm2d(256)\n",
        "    self.bn6 = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.bn10 = nn.BatchNorm2d(1)\n",
        "    self.bn11 = nn.BatchNorm2d(16)\n",
        "    self.bn12 = nn.BatchNorm2d(32)\n",
        "    self.bn13 = nn.BatchNorm2d(64)\n",
        "    self.bn14 = nn.BatchNorm2d(128)\n",
        "    self.bn15 = nn.BatchNorm2d(256)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x1 = self.conv1(x)\n",
        "    x1 = F.leaky_relu(x1, 0.2)\n",
        "    x1 = self.bn1(x1)\n",
        "    x2 = self.conv2(x1)\n",
        "    x2 = F.leaky_relu(x2, 0.2)\n",
        "    x2 = self.bn2(x2)\n",
        "    x3 = self.conv3(x2)\n",
        "    x3 = F.leaky_relu(x3, 0.2)\n",
        "    x3 = self.bn3(x3)\n",
        "    x4 = self.conv4(x3)\n",
        "    x4 = F.leaky_relu(x4, 0.2)\n",
        "    x4 = self.bn4(x4)\n",
        "    x5 = self.conv5(x4)\n",
        "    x5 = F.leaky_relu(x5, 0.2)\n",
        "    x5 = self.bn5(x5)\n",
        "    x6 = self.conv6(x5)\n",
        "    x6 = F.leaky_relu(x6, 0.2)\n",
        "    x6 = self.bn6(x6)\n",
        "\n",
        "    d1 = self.convT1(x6, output_size= x5.shape[2:])\n",
        "    d1 = self.conv7(torch.cat((d1, x5), dim=1))\n",
        "    d1 = F.relu(d1)\n",
        "    d1 = self.bn15(d1)\n",
        "    d1 = self.dropout(d1)\n",
        "    d2 = self.convT2(d1, output_size=x4.shape[2:])\n",
        "    d2 = self.conv8(torch.cat((d2, x4), dim=1))\n",
        "    d2 = F.relu(d2)\n",
        "    d2 = self.bn14(d2)\n",
        "    d2 = self.dropout(d2)\n",
        "    d3 = self.convT3(d2, output_size=x3.shape[2:])\n",
        "    d3 = self.conv9(torch.cat((d3, x3), dim=1))\n",
        "    d3 = F.relu(d3)\n",
        "    d3 = self.bn13(d3)\n",
        "    d3 = self.dropout(d3)\n",
        "    d4 = self.convT4(d3, output_size=x2.shape[2:])\n",
        "    d4 = self.conv10(torch.cat((d4, x2), dim=1))\n",
        "    d4 = F.relu(d4)\n",
        "    d4 = self.bn12(d4)\n",
        "    d5 = self.convT5(d4, output_size=x1.shape[2:])\n",
        "    d5 = self.conv11(torch.cat((d5, x1), dim=1))\n",
        "    d5 = F.relu(d5)\n",
        "    d5 = self.bn11(d5)\n",
        "    d6 = self.convT6(d5, output_size=x.shape[2:])\n",
        "    d6 = torch.sigmoid(d6)\n",
        "    d6 = self.bn10(d6)\n",
        "    out1 = torch.mul(d6, x)\n",
        "    return out1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXpocDn37F26"
      },
      "source": [
        "train_path = '/content/drive/MyDrive/UGP/UGP/ugp data/train/'\n",
        "train_files = np.array(sorted(list(set(map(lambda x : x, os.listdir(train_path))))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-luB2SiK7HV3",
        "outputId": "d605b250-f9f3-4b88-f0d3-1473f12a2460"
      },
      "source": [
        "val_files = np.load('/content/drive/MyDrive/UGP/UGP/val.npz.npy')\n",
        "val_files = [x.split('/')[-1] for x in val_files]\n",
        "\n",
        "t = []\n",
        "for x in train_files:\n",
        "  if(x not in val_files and x.split('.')[-1]=='npz'):\n",
        "    t.append(x)\n",
        "train_files = list(set(t))\n",
        "\n",
        "len(val_files), len(train_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 90)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs04Fa9H7Ir4"
      },
      "source": [
        "STEM = 'vocal'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpsDiJn-7Kep"
      },
      "source": [
        "class NewTrainGenerator(Dataset):\n",
        "  def __init__(self, sup_files, hitl_files, sup_basepath, hitl_basepath, stem, batch_size, hitl_size, hitl_iter):\n",
        "    self.sup_files = sup_files\n",
        "    self.hitl_files = hitl_files\n",
        "    self.sup_basepath = sup_basepath\n",
        "    self.hitl_basepath = hitl_basepath\n",
        "    self.stem = stem\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.hitl_size = hitl_size\n",
        "    self.hitl_iter = hitl_iter\n",
        "\n",
        "    self.hitl_mapping = []\n",
        "    self.hitl_mapping_file = []\n",
        "\n",
        "    x = []\n",
        "    for r,f in enumerate(self.hitl_files):\n",
        "      for k in range(self.hitl_iter):\n",
        "        data = np.load(self.hitl_basepath + f)['mix']\n",
        "        l = data.shape[-1]-512\n",
        "        for j in range(self.hitl_size):\n",
        "          idx = np.random.randint(0, l)\n",
        "          x.append(idx)\n",
        "        self.hitl_mapping.append(x)\n",
        "        self.hitl_mapping_file.append([r for i in range(self.hitl_size)])\n",
        "        x = []\n",
        "\n",
        "    self.sup_mapping = []\n",
        "    self.sup_mapping_file = []\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    x = []\n",
        "    f = open('/content/drive/MyDrive/UGP/UGP/data_len', 'rb')\n",
        "    out = pickle.load(f)\n",
        "    f.close()\n",
        "    for r,f in enumerate(self.sup_files):\n",
        "      l = out[f.split('/')[-1]]-512\n",
        "      for j in range(10):\n",
        "        idx = np.random.randint(0, l)\n",
        "        x.append(idx)\n",
        "      self.sup_mapping.append(x)\n",
        "      self.sup_mapping_file.append(r)\n",
        "      x = []\n",
        "    \n",
        "    x = []\n",
        "    for i in range(len(self.hitl_mapping)):\n",
        "      for m in range(self.batch_size - self.hitl_size):\n",
        "        j = np.random.randint(len(self.sup_mapping))\n",
        "        k = np.random.randint(len(self.sup_mapping[j]))\n",
        "        self.hitl_mapping[i].append(k)\n",
        "        self.hitl_mapping_file[i].append(self.sup_mapping_file[j])\n",
        "    \n",
        "    self.idx_to_idx = np.arange(len(self.hitl_mapping))\n",
        "    np.random.shuffle(self.idx_to_idx)\n",
        "     \n",
        "  def __len__(self):\n",
        "    return(len(self.hitl_mapping))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    X, Y = [], []\n",
        "    idx = self.idx_to_idx[idx]\n",
        "\n",
        "    for i in range(self.hitl_size):\n",
        "      mask_data = np.load(self.hitl_basepath + self.hitl_files[self.hitl_mapping_file[idx][i]])\n",
        "      norm = np.max(mask_data['mix'])\n",
        "      j = self.hitl_mapping[idx][i]\n",
        "      X.append(np.expand_dims(mask_data['mix'][:-1, j:j+512], axis=0) / norm)\n",
        "      Y.append(np.expand_dims(mask_data[self.stem][:-1, j:j+512], axis=0) / norm)\n",
        "    \n",
        "    for i in range(self.batch_size-self.hitl_size):\n",
        "      mask_data = np.load(self.sup_basepath + self.sup_files[self.hitl_mapping_file[idx][i+self.hitl_size]])\n",
        "      norm = np.max(mask_data['mix'])\n",
        "      j = self.hitl_mapping[idx][i+self.hitl_size]\n",
        "      X.append(np.expand_dims(mask_data['mix'][:-1, j:j+512], axis=0) / norm)\n",
        "      Y.append(np.expand_dims(mask_data[self.stem][:-1, j:j+512], axis=0) / norm)\n",
        "    \n",
        "    return(torch.FloatTensor(X), torch.FloatTensor(Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_deMEt7Z8ceR"
      },
      "source": [
        "STEM = 'vocal'\n",
        "\n",
        "def eval(files, model, savePath, step_size):\n",
        "  \n",
        "  SR =  22050\n",
        "  window_size = 2048\n",
        "  hop_length = 512\n",
        "  orig_sr=44100\n",
        "\n",
        "  for f in files:\n",
        "    fname = f.split('.')[:-1]\n",
        "    strname = ''\n",
        "    for i in fname:\n",
        "      if(strname == ''):\n",
        "        strname += i\n",
        "      else:\n",
        "        strname += ('.'+i)\n",
        "    file_name = strname+'.stem'\n",
        "    print(savePath+file_name)\n",
        "    try:\n",
        "      os.mkdir(savePath+file_name)\n",
        "    except:\n",
        "      # print(\"pepe\")\n",
        "      pass\n",
        "    data = np.load('/content/drive/MyDrive/UGP/UGP/ugp data/train/'+f)\n",
        "    prediction = np.zeros(data['mix'].shape)\n",
        "    x = np.ceil(np.ceil(data['mix'].shape[1]/512)/32)\n",
        "    curr_idx = 0\n",
        "    norm = data['mix'].max()\n",
        "    for i in range(int(x)):\n",
        "      X = []\n",
        "      flag = 0\n",
        "      while(len(X)!=32 and curr_idx < data['mix'].shape[1]-511):\n",
        "        X.append(data['mix'][:-1, curr_idx:curr_idx+512] / norm)\n",
        "        curr_idx += 512\n",
        "      if(len(X)!=32 and data['mix'].shape[1]!=curr_idx and data['mix'].shape[1]-curr_idx<512):\n",
        "        X.append(data['mix'][:-1, -512:]/norm)\n",
        "        flag = 1\n",
        "\n",
        "      X = np.array(X)\n",
        "      X = torch.from_numpy(X)\n",
        "      X = torch.unsqueeze(X, dim=1).to(device)\n",
        "      # X = X.to(device)\n",
        "      # print(X.shape)\n",
        "      model.eval()\n",
        "      preds = model(X).detach().cpu()\n",
        "      preds = np.squeeze(preds, axis=1)\n",
        "      preds = np.expand_dims(preds, axis=-1)\n",
        "      # print(preds.shape)\n",
        "      if(flag):\n",
        "        for j in range(i*32, data['mix'].shape[1]//512):\n",
        "          prediction[:-1, j*512 : (j+1)*512] = np.squeeze(preds[j-(i*32)], axis=-1)\n",
        "        prediction[:-1, data['mix'].shape[1]//512*512:] = np.squeeze(preds[-1][:, -(data['mix'].shape[1]-((data['mix'].shape[1]//512)*512)):], axis=-1)\n",
        "      else:\n",
        "        for j in range(i*32, (i+1)*32):\n",
        "          prediction[:-1, j*512 : (j+1)*512] = np.squeeze(preds[j-(i*32)], axis=-1)\n",
        "\n",
        "\n",
        "    # print(f'Stem Extracted for file')\n",
        "    f_name = '/content/drive/MyDrive/UGP/UGP/ugp data/train/'+f[:-4]+'.stem.mp4'\n",
        "    # print(f_name)\n",
        "    S, rate = st.read_stems(f_name)\n",
        "    y_mix_o = librosa.to_mono(S[0].T)  \n",
        "    # y_mix =  librosa.core.resample(y_mix_o,orig_sr,SR)\n",
        "    # spec = librosa.stft(y_mix,n_fft=window_size,hop_length=hop_length)\n",
        "    # mag, phase = librosa.magphase(spec)\n",
        "    prediction *= norm\n",
        "    phase = np.load('/content/drive/MyDrive/UGP/UGP/ugp data/train/phase/'+f[:-4]+\".npz\")[\"arr_0\"]\n",
        "    y = librosa.istft(prediction*phase,win_length=window_size,hop_length=hop_length)\n",
        "    y = librosa.resample(y,SR,orig_sr, fix=True)\n",
        "\n",
        "    file_path = (savePath+strname+'.stem/'+STEM+'.wav')\n",
        "    sf.write(file_path, y, orig_sr)\n",
        "\n",
        "    accom = y_mix_o-y\n",
        "    file_path = (savePath+strname+'.stem/'+'accom.wav')\n",
        "    sf.write(file_path, accom, orig_sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgbHMjFQ8eUy"
      },
      "source": [
        "def evaluate(files):\n",
        "  est_path = '/content/drive/MyDrive/UGP/UGP/preds/'\n",
        "  src_path = '/content/drive/MyDrive/UGP/UGP/stems/'\n",
        "  s = []\n",
        "  for f in files:\n",
        "    fname = f.split('.')[:-1]\n",
        "    strname = ''\n",
        "    for i in fname:\n",
        "      if(strname == ''):\n",
        "        strname += i\n",
        "      else:\n",
        "        strname += ('.'+i)\n",
        "    srcs = np.array([sf.read(src_path+strname+'_accom.wav')[0],\n",
        "            sf.read(src_path+strname+'_vocal.wav')[0]])\n",
        "    # drum = sf.read(est_path + f.split('.')[0]+'.stem/drum.wav')[0]\n",
        "    # bass = sf.read(est_path + f.split('.')[0]+'.stem/bass.wav')[0]\n",
        "    vocal = sf.read(est_path + strname+'.stem/vocal.wav')[0]\n",
        "    # other = sf.read(est_path + f.split('.')[0]+'.stem/rest.wav')[0]\n",
        "    accom = sf.read(est_path + strname+'.stem/accom.wav')[0]\n",
        "    y = np.array([accom, vocal])\n",
        "    # print(museval.metrics.validate(np.expand_dims(srcs, -1), np.expand_dims(y, -1)))\n",
        "    # y = np.concatenate([y, np.zeros((y.shape[0], srcs.shape[1]-y.shape[1]))], axis=-1)\n",
        "    score = museval.metrics.bss_eval(np.expand_dims(srcs, -1), np.expand_dims(y, -1))\n",
        "    # print(type(score[0]))\n",
        "    s.append(score)\n",
        "    # sdr += score[0]\n",
        "    # sir += score[2]\n",
        "    # sar += score[3]\n",
        "\n",
        "  return(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfnmiTQulUf6"
      },
      "source": [
        "Complete pipeline for finetuning base model on synthetic music track created from partial annotations of a single song. First the dataloader is defined with appropriate hyperparameter values. Next the base model is finetuned and finally the finetuned model is evaluated on mean and median SDR metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm1aNF4S7cdx",
        "outputId": "1b8eb750-169a-495a-bf53-4277d2e13b2a"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print(i)\n",
        "  np.random.seed(42)\n",
        "  torch.random.seed = 42\n",
        "\n",
        "  files = ['hitl16.npz']\n",
        "\n",
        "  train_datagen = NewTrainGenerator(train_files, files, './data/', './data/', STEM, BATCH_SIZE, 1, i+1)\n",
        "\n",
        "  model = torch.load(f'/content/drive/MyDrive/UGP/UGP/baseline_models/pytorch_hitl_new.pt').to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr= 0.00001)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=2e-5)\n",
        "  mae = torch.nn.L1Loss()\n",
        "  mae.to(device)\n",
        "  for epoch in range(1):\n",
        "    model.train()\n",
        "    t_loss = 0\n",
        "    for batch in train_datagen:\n",
        "      X, Y_mask = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "      output = model(X)\n",
        "      # print(output.shape)\n",
        "\n",
        "      loss = mae(output, Y_mask)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      t_loss += loss.item()\n",
        "\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "    # print(f'Train Loss : {t_loss/len(train_datagen)}')\n",
        "\n",
        "    if(epoch==0 or epoch==4 or epoch==9):\n",
        "      eval([\"Phre The Eon - Everybody's Falling Apart.npz\"], model, '/content/drive/MyDrive/UGP/UGP/preds/', 512)\n",
        "      s = evaluate([\"Phre The Eon - Everybody's Falling Apart.npz\"])\n",
        "      sdr = np.array([0,0], dtype=np.float64)\n",
        "      sir = np.array([0,0], dtype=np.float64)\n",
        "      sar = np.array([0,0], dtype=np.float64)\n",
        "\n",
        "      for track in s:\n",
        "        sdr += np.array([np.nanmedian(track[0][0]), np.nanmedian(track[0][1])])\n",
        "        sir += np.array([np.nanmedian(track[2][0]), np.nanmedian(track[2][1])])\n",
        "        sar += np.array([np.nanmedian(track[3][0]), np.nanmedian(track[3][1])])\n",
        "\n",
        "      print(sdr)\n",
        "\n",
        "      sdr = np.array([0,0], dtype=np.float64)\n",
        "      sir = np.array([0,0], dtype=np.float64)\n",
        "      sar = np.array([0,0], dtype=np.float64)\n",
        "\n",
        "      for track in s:\n",
        "        sdr += np.array([np.nanmean(track[0][0]), np.nanmean(track[0][1])])\n",
        "        sir += np.array([np.nanmean(track[2][0]), np.nanmean(track[2][1])])\n",
        "        sar += np.array([np.nanmean(track[3][0]), np.nanmean(track[3][1])])\n",
        "\n",
        "      print(sdr)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.02324905  6.7520741 ]\n",
            "[18.49024168  1.64544417]\n",
            "1\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.02197728  6.79287478]\n",
            "[18.51309886  1.71821654]\n",
            "2\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.06721867  6.79709608]\n",
            "[18.57782665  1.89556815]\n",
            "3\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.0702291   6.78218315]\n",
            "[18.58704964  1.93310067]\n",
            "4\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.0929759   6.88351916]\n",
            "[18.67160189  2.17256917]\n",
            "5\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.11951212  6.96062252]\n",
            "[18.65717853  2.07353363]\n",
            "6\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.13323324  6.93427173]\n",
            "[18.68422557  2.16947657]\n",
            "7\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.14053003  6.8714642 ]\n",
            "[18.63781675  2.03114325]\n",
            "8\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.12720589  6.8971147 ]\n",
            "[18.70788488  2.23507766]\n",
            "9\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.13692172  6.9372011 ]\n",
            "[18.80740392  2.58742341]\n",
            "10\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.08683164  6.92024841]\n",
            "[18.82851619  2.6878311 ]\n",
            "11\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.05915743  6.89790181]\n",
            "[18.84178025  2.80374244]\n",
            "12\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.05313811  6.86889919]\n",
            "[18.85514196  2.94840961]\n",
            "13\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.03481544  6.90088208]\n",
            "[18.82745466  2.72206684]\n",
            "14\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.09376356  6.85559485]\n",
            "[18.88561629  2.86938341]\n",
            "15\n",
            "/content/drive/MyDrive/UGP/UGP/preds/Phre The Eon - Everybody's Falling Apart.stem\n",
            "[15.0502624   6.84512964]\n",
            "[18.91801937  3.31509072]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}